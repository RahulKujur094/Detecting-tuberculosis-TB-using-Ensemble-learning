{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11200393,"sourceType":"datasetVersion","datasetId":6993117}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport numpy as np\n\n# Configuration\nDATA_DIR = \"/kaggle/input/tb-data/tbdata/Training_Dataset_TB\"\nTEST_DIR = \"/kaggle/input/tb-data/tbdata/Test_Dataset_TB\"\nCSV_PATH = \"/kaggle/input/tb-data/tbdata/TB_train.csv\"\nBATCH_SIZE = 32\nIMAGE_SIZE = 224\nEPOCHS = 15\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load and prepare data\ndf = pd.read_csv(CSV_PATH)\ndf = df.rename(columns={'Target': 'label'})  # Keep ID column as is\n\n# Split data\ntrain_df, val_df = train_test_split(df, test_size=0.15, stratify=df['label'], random_state=42)\n\n# Calculate class weights\nclass_counts = df['label'].value_counts()\nclass_weights = class_counts[0] / class_counts[1]\nprint(f\"Class weights: {class_weights:.2f}\")\n\n# Image transformations\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(IMAGE_SIZE),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Dataset class\nclass TBXRayDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.df.iloc[idx]['ID'])\n        image = Image.open(img_path).convert('RGB')\n        label = torch.tensor(self.df.iloc[idx]['label'], dtype=torch.float32)\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, label\n\n# Create datasets and dataloaders\ntrain_dataset = TBXRayDataset(train_df, DATA_DIR, train_transform)\nval_dataset = TBXRayDataset(val_df, DATA_DIR, val_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n# Model setup\nweights = models.EfficientNet_B3_Weights.IMAGENET1K_V1\nmodel = models.efficientnet_b3(weights=weights)\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, 1)\nmodel = model.to(DEVICE)\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weights]).to(DEVICE))\noptimizer = optim.Adam(model.parameters(), lr=3e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)\n\n# Training loop\nbest_f1 = 0\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in train_loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        \n        optimizer.zero_grad()\n        outputs = model(images).squeeze()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        \n    # Validation\n    model.eval()\n    val_preds = []\n    val_true = []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images).squeeze()\n            \n            val_loss += criterion(outputs, labels).item()\n            preds = torch.sigmoid(outputs).cpu().numpy()\n            \n            val_preds.extend(preds)\n            val_true.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    val_preds_bin = np.array(val_preds) > 0.5\n    f1 = f1_score(val_true, val_preds_bin)\n    acc = accuracy_score(val_true, val_preds_bin)\n    prec = precision_score(val_true, val_preds_bin)\n    rec = recall_score(val_true, val_preds_bin)\n    \n    epoch_loss = running_loss / len(train_loader.dataset)\n    val_loss = val_loss / len(val_loader)\n    \n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    print(f\"Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f}\")\n    print(f\"F1: {f1:.4f} | Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f}\")\n    \n    # Save best model\n    if f1 > best_f1:\n        best_f1 = f1\n        torch.save(model.state_dict(), \"best_model2.pth\")\n        print(\"Saved best model!\")\n\n# Generate predictions\nmodel.load_state_dict(torch.load(\"best_model2.pth\"))\nmodel.eval()\n\ntest_files = [f for f in os.listdir(TEST_DIR) if f.endswith('.png')]\ntest_preds = []\n\nwith torch.no_grad():\n    for file in test_files:\n        img_path = os.path.join(TEST_DIR, file)\n        image = Image.open(img_path).convert('RGB')\n        image = val_test_transform(image).unsqueeze(0).to(DEVICE)\n        \n        output = torch.sigmoid(model(image)).item()\n        test_preds.append({\n            'ID': file,\n            'Target': 1 if output > 0.5 else 0\n        })\n\n# Create submission file\nsubmission_df = pd.DataFrame(test_preds)\nsubmission_df.to_csv('tb_predictions2.csv', index=False)\nprint(\"Submission file created: tb_predictions2.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:58:59.770067Z","iopub.execute_input":"2025-03-28T21:58:59.770367Z","iopub.status.idle":"2025-03-28T22:00:55.475012Z","shell.execute_reply.started":"2025-03-28T21:58:59.770344Z","shell.execute_reply":"2025-03-28T22:00:55.473455Z"}},"outputs":[],"execution_count":null}]}